# -*- coding: utf-8 -*-
"""deteccionV6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/122EvSncAIhqcAtFjYsi-aB5UpoGfT1R3

# **1. Imagen de características especio temporales**
"""

import cv2
import tensorflow as tf
from tensorflow.keras.layers import Flatten
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

base_model=tf.keras.applications.VGG19(
                include_top=False,
                weights='imagenet',
                input_shape=(224,224,3)
            )

model=Sequential()
model.add(base_model)
model.add(Flatten())

def readVideo(path):
    frames=[]
    vidcap=cv2.VideoCapture(path)
    success,image=vidcap.read()

    while success:
        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
        frames.append(cv2.resize(image,(224,224)))
        success,image=vidcap.read()

    return frames

frames=readVideo("/content/drive/MyDrive/Clasificación-Videos/Fase3/No_Robo_omitir_2frames/NR (1)_rec.mp4")

len(frames)

plt.imshow(frames[0])

frames[0].shape

def convFeaatureImage(frames):
    conv_features=model.predict(np.array(frames))
    return np.array(conv_features)

img_features = convFeaatureImage(frames)

img_features.shape

"""# **2. Generación del dataset de entrenamiento "inducción al error"**"""

import glob
import os

paths=["/content/drive/MyDrive/Clasificación-Videos/Fase3/Robo_omitir_2frames/",
      "/content/drive/MyDrive/Clasificación-Videos/Fase3/No_Robo_omitir_2frames/"]

print(len("/content/drive/MyDrive/Clasificación-Videos/Fase3/Robo_omitir_2frames/"))

print(len("/content/drive/MyDrive/Clasificación-Videos/Fase3/No_Robo_omitir_2frames/"))

def resize_zeros(img_features,maxFrames):
    rows,col=img_features.shape
    zeroMatrix=np.zeros((maxFrames-rows,col))
    return np.concatenate((img_features,zeroMatrix),axis=0)

resize_zeros(img_features,500).shape

maxFrames=500

"""# **3. Escribir TFrecords**"""

import random
from tqdm import tqdm
import glob

# Fijar la semilla para la reproducibilidad
random.seed(0)

# Tamaño del lote de videos consecutivos
batch_size = 50

# Solicitar al usuario el número de lote
#lote = int(input("Ingrese el número de lote: "))
lote = 19

label_videoPath = []

for label, path in enumerate(paths):
    files = glob.glob(path + '*.mp4')
    files.sort()  # Ordena los archivos para asegurar que los lotes sean consecutivos

    start_index = (lote - 1) * batch_size
    end_index = lote * batch_size

    if len(files) >= end_index:
        batch_files = files[start_index:end_index]
        label_videoPath.extend([(label, file) for file in batch_files])
    else:
        print(f"No hay suficientes archivos para el lote {lote} en la carpeta {path}")

# Imprimir los resultados para verificar
for label, video in label_videoPath:
    print(f"Label: {label}, Video: {video}")

random.shuffle(label_videoPath)

len(label_videoPath)

eval_files=[file for label,file in label_videoPath[int(0.90*len(label_videoPath))::]]
test_files=[file for label,file in label_videoPath[int(0.70*len(label_videoPath)):int(0.9*len(label_videoPath))]]

print(len(eval_files),len(test_files))

def createTfExample(img_feature,label):
    sample={ 'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[img_features.shape[0]])),
             'width':  tf.train.Feature(int64_list=tf.train.Int64List(value=[img_features.shape[1]])),
             'image':  tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_features.tobytes()])),
             'label':  tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
    }

    return tf.train.Example(features=tf.train.Features(feature=sample))

train_writer=tf.io.TFRecordWriter('train.record')
eval_writer=tf.io.TFRecordWriter('eval.record')
test_writer=tf.io.TFRecordWriter('test.record')
for label, file in tqdm(label_videoPath):
    frames=readVideo(file)
    if len(frames)<maxFrames:
        img_features=convFeaatureImage(frames)
        img_features=resize_zeros(img_features,maxFrames)
        example=createTfExample(img_features,label)
        if file in eval_files:
            eval_writer.write(example.SerializeToString())
        elif file in test_files:
            test_writer.write(example.SerializeToString())
        else:
            train_writer.write(example.SerializeToString())
train_writer.close()
eval_writer.close()
test_writer.close()

"""# **4. Leer los TFrecords**"""

import tensorflow as tf

dataset_train=tf.data.TFRecordDataset("train.record")
dataset_eval=tf.data.TFRecordDataset("eval.record")
dataset_test=tf.data.TFRecordDataset("test.record")

feature_format={
    'height': tf.io.FixedLenFeature([],tf.int64),
    'width':  tf.io.FixedLenFeature([],tf.int64),
    'image':  tf.io.FixedLenFeature([],tf.string),
    'label':  tf.io.FixedLenFeature([],tf.int64)
}

def get_dataFromBinary(proto):
    f=tf.io.parse_single_example(proto,feature_format)
    height=tf.cast(f['height'],tf.int32)
    width=tf.cast(f['width'],tf.int32)
    img=tf.io.decode_raw(f['image'],tf.float64)
    img=tf.reshape(img,[height,width])
    label=f['label']
    return (img,label)

def proc_dataset(dataset):
    dataset=dataset.map(get_dataFromBinary)
    dataset=dataset.batch(5)
    return dataset

dataset_train=proc_dataset(dataset_train)
dataset_eval=proc_dataset(dataset_eval)
dataset_test=proc_dataset(dataset_test)

"""# **5. Definición del modelo**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPooling1D, Dropout, Conv3D, MaxPooling3D
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import GRU
from tensorflow.keras.layers import Conv1D, MaxPooling1D, SimpleRNN
from tensorflow.keras.layers import Bidirectional, GRU

feature_size = 25088

def cnn_model():
    model = Sequential()
    model.add(Conv1D(filters=128,kernel_size=4,input_shape=(500,25088),activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=64,kernel_size=4,activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(filters=32,kernel_size=4,activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(64,activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(1,activation='sigmoid'))
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),loss='binary_crossentropy',metrics=['accuracy'])
    return model

modelCnn=cnn_model()

modelCnn.summary()

checkpoint = ModelCheckpoint('checkpoint/model.{epoch:d}.h5', save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True, save_freq='epoch')

modelCnn.fit(dataset_train,
            epochs=10,
            batch_size=15,
            validation_data=dataset_eval,
            callbacks=[checkpoint])

modelCnn.load_weights("./checkpoint/model.1.h5")

"""# **6. Evaluación del modelo**"""

evaluation_results = modelCnn.evaluate(dataset_eval)
print("Pérdida en el conjunto de eval:", evaluation_results[0])
print("Precisión en el conjunto de eval:", evaluation_results[1])

evaluation_results = modelCnn.evaluate(dataset_test)
print("Pérdida en el conjunto de test:", evaluation_results[0])
print("Precisión en el conjunto de test:", evaluation_results[1])

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

def get_predictions_and_labels(dataset, model):
    y_true = []
    y_pred = []
    for img, label in dataset:
        preds = model.predict(img)
        y_true.extend(label.numpy())
        y_pred.extend((preds >= 0.5).astype(int))
    return y_true, y_pred

# Obtener etiquetas verdaderas y predicciones para conjuntos de entrenamiento, evaluación y prueba
y_true_test, y_pred_test = get_predictions_and_labels(dataset_test, modelCnn)

# Generar informes de clasificación
report_test = classification_report(y_true_test, y_pred_test, target_names=['No Robo', 'Robo'])

# Imprimir informes de clasificación y matrices de confusión
print("\nInforme de Clasificación para el Conjunto de Prueba:")
print(report_test)

cm = confusion_matrix(y_true_test, y_pred_test)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Delictiva', 'Delictiva'], yticklabels=['No Delictiva', 'Delictiva'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import imageio
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage.transform import resize
from IPython.display import HTML

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Clasificación-Videos/Videos_pruebas

def display_video(video):
    fig = plt.figure(figsize=(3,3))  #Display size specification

    mov = []
    for i in range(len(video)):  #Append videos one by one to mov
        img = plt.imshow(video[i], animated=True)
        plt.axis('off')
        mov.append([img])

    #Animation creation
    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)

    plt.close()
    return anime

"""# **7. Predicción para un video**"""

!ls /content/drive/MyDrive/Clasificación-Videos/Videos_pruebas/

video = imageio.get_reader('Robo_prueba.mp4')  #Loading video
video = [resize(frame, (224, 224))[..., :3] for frame in video]    #Size adjustment (if necessary)
HTML(display_video(video).to_html5_video())  #Inline video display in HTML5

frames=readVideo('/content/drive/MyDrive/Clasificación-Videos/Videos_pruebas/Robo_prueba.mp4')
img_features=convFeaatureImage(frames)
img_features=resize_zeros(img_features,maxFrames)
img_features.shape

if modelCnn.predict(np.array([img_features]))[0]>=0.5:
    print("El video es de ROBO")
else:
    print("El video no es de ROBO")